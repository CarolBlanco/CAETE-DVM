# This script contains functions to read binary output
# and create gridded and table outputs.
# Author: Joao Paulo Darela Filho

from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Union, Collection, Tuple, Dict, List
import os

from numpy.typing import NDArray
import numpy as np
import pandas as pd

from worker import worker
from region import region
from caete import grd_mt

from _geos import pan_amazon_region, get_region

#TODO: implement region configuration
if pan_amazon_region is None:
    raise ValueError("pan_amazon_region is not defined or imported correctly")

# Get the region of interest
ymin, ymax, xmin, xmax = get_region(pan_amazon_region)

# IO
processed_data = Path("out_data")
os.makedirs(processed_data, exist_ok=True)

model_results: Path = Path("./pan_amazon_hist_result.psz")

# Load the region file
reg:region = worker.load_state_zstd(model_results)


def get_args(variable: Union[str, Collection[str]])-> Collection[str]:
    """helper function to ensure that the variable is a collection.

    Args:
        variable (Union[str, Collection[str]]): variable name(s) to be read

    Returns:
        Collection[str]: a collection with the variable name(s)
    """
    if isinstance(variable, str):
        variable = [variable,]
    if isinstance(variable, Collection):
        return variable


def print_spins(r:region, gridcell=0):
    """Prints the available spin slices for a gridcell in the region"""
    return r[gridcell].print_available_periods()


def print_variables(r:region):
    """Prints the available variables for each gridcell in the region"""
    reg[0]._get_daily_data("DUMMY", 1, pp=True)

#=========================================
# Functions dealing with gridded outputs
#=========================================
class gridded_data:


    @staticmethod
    def read_grd(grd:grd_mt,
                 variables: Union[str, Collection[str]],
                 spin_slice: Union[int, Tuple[int, int], None]
                 ) -> Tuple[NDArray, Union[Dict, NDArray, List, Tuple], Union[int, float], Union[int, float]]:
        """helper function to read gridcell output data.

        Args:
            grd (_type_): grd_mt
            variables (Collection[str]): which variables to read from the gridcell
            spin_slice (Union[int, Tuple[int, int], None]): which spin slice to read

        Returns:
            _type_: _description_
        """
        data = grd._get_daily_data(get_args(variables), spin_slice, return_time=True) # returns a tuple with data and time Tuple[NDArray, NDArray]
        time = data[-1]
        data = data[0]
        return time, data, grd.y, grd.x


    @staticmethod
    def aggregate_region_data(r: region,
                variables: Union[str, Collection[str]],
                spin_slice: Union[int, Tuple[int, int], None] = None
                )-> Dict[str, NDArray]:
        """_summary_

        Args:
            r (region): a region object

            variables (Union[str, Collection[str]]): variable names to read

            spin_slice (Union[int, Tuple[int, int], None], optional): which spin slice to read.
            Defaults to None, read all available data. Consumes a lot of memory.

        Returns:
            dict: a dict with the following keys: time, coord, data holding data to be transformed
            necessary to create masked arrays and subsequent netCDF files.
        """

        output = []
        nproc = min(len(r), r.nproc//2)
        nproc = max(1, nproc) # Ensure at least one process is used
        with ThreadPoolExecutor(max_workers=nproc) as executor:
            futures = [executor.submit(gridded_data.read_grd, grd, variables, spin_slice) for grd in r]
            for future in futures:
                output.append(future.result())

        # Finalize the data object
        raw_data = np.array(output, dtype=object)
        # Reoeganize resources
        time = raw_data[:,0][0] # We assume all slices have the same time, thus we get the first one
        coord = raw_data[:,2:4][:].astype(np.int64) # 2D matrix of coordinates (y(lat), x(lon))}
        data = raw_data[:,1][:]  # array of dicts, each dict has the variables as keys and the time series as values

        if isinstance(variables, str):
            dim_names = ["time", "coord", variables]
        else:
            dim_names = ["time", "coord", "data"]

        return dict(zip(dim_names, (time, coord, data)))


    @staticmethod
    def create_masked_arrays(data: dict):
        """ Reads a dict generated by aggregate_region_data and reorganize the data
        as masked_arrays with shape=(time, lat, lon) for each variable

        Args:
            data (dict): a dict generated by aggregate_region_data

        Returns:
            _type_: a tuple with a list of masked_arrays (for each variable)
            and the time array.
        """
        time = data["time"]
        coords = data["coord"]
        # If it is only one variable, we put
        variables = list(data["data"][0].keys()) # holds variable names being processed

        dim = data["data"][0][variables[0]].shape

        # TODO: manage 2D and 3D arrays
        assert len(dim) == 1, "Only 1D array allowed"
        arrays_dict = data["data"][:]

        # Read dtypes
        dtypes = []
        for var in variables:
            # We assume all gridcells have the same variables, thus we get the first one
            dtypes.append(arrays_dict[0][var].dtype)

        arrays = []
        for i, var in enumerate(variables):
            arrays.append(np.ma.masked_all(shape=(dim[0], 360, 720), dtype=dtypes[i]))

        for i, var in enumerate(variables):
            for j in range(len(coords)):
                arrays[i][:, coords[j][0], coords[j][1]] = arrays_dict[j][var]
        # Crop the arrays to the region of interest
        arrays = [a[:, ymin:ymax, xmin:xmax] for a in arrays]

        return arrays, time


    @staticmethod
    def create_masked_arrays2D(data: dict):
        """ Reads a dict generated by aggregate_region_data and reorganize the data
        as masked_arrays with shape=(time, lat, lon) for each variable

        Args:
            data (dict): a dict generated by aggregate_region_data

        Returns:
            _type_: a tuple with a list of masked_arrays (for each variable),
            the time array, and the array names.
        """
        time = data["time"]
        coords = data["coord"]

        assert "data" in data.keys(), "The input dict must contain the 'data' keyword"
        assert isinstance(data["data"][0], dict), "Data must be a dict"
        variables = list(data["data"][0].keys())  # holds variable names being processed

        arrays_dict = data["data"][:]

        # Read dtypes
        dtypes = []
        for var in variables:
            dtypes.append(arrays_dict[0][var].dtype)

        # Allocate the arrays
        arrays = []
        array_names = []
        for i, var in enumerate(variables):
            dim = arrays_dict[0][var].shape
            if len(dim) == 1:
                arrays.append(np.ma.masked_all(shape=(dim[0], 360, 720), dtype=dtypes[i]))
                array_names.append(var)
            elif len(dim) == 2:
                ny, nx = dim
                for k in range(ny):
                    arrays.append(np.ma.masked_all(shape=(nx, 360, 720), dtype=dtypes[i]))
                    array_names.append(f"{var}_{k + 1}")
        # Fill the arrays
        array_index = 0
        for i, var in enumerate(variables):
            for j in range(len(coords)):
                if len(arrays_dict[j][var].shape) == 1:
                    arrays[array_index][:, coords[j][0], coords[j][1]] = arrays_dict[j][var]
                elif len(arrays_dict[j][var].shape) == 2:
                    ny, nx = arrays_dict[j][var].shape
                    for k in range(ny):
                        arrays[array_index + k][:, coords[j][0], coords[j][1]] = arrays_dict[j][var][k, :]
            array_index += ny if len(arrays_dict[j][var].shape) == 2 else 1

        # Crop the arrays to the region of interest
        arrays = [a[:, ymin:ymax, xmin:xmax] for a in arrays]

        return arrays, time, array_names


    @staticmethod
    def save_netcdf(data: dict, output_path: Path, file_name: str):
        pass


# ======================================
# Functions dealing with metacommunity outputs
# ======================================
class metacommunity_data:

    #TODO: implement these methods
    @staticmethod
    def read_annual_data(r: region, variables: Union[str, Collection[str]]) -> Dict[str, NDArray]:
        """Reads annual data from the metacommunities region

        Args:
            r (region): a region object
            variables (Union[str, Collection[str]]): variable names to read

        Returns:
            dict: a dict with the following keys: time, coord, data holding data to be transformed
            necessary to create masked arrays and subsequent netCDF files.
        """
        data = {}
        # for grd in r:
        #     years = grd._get_nyears()
        #     gridcell_data = []
        #     for i,y in enumerate(years):
        #         year_data = grd._fetch_metacommunity_data(y)
        #         gridcell_data.append(year_data)
        #     data[grd.xyname] = gridcell_data
        return data


# ======================================
# Functions dealing with table outputs
# ======================================
class table_data:

    @staticmethod
    def make_df(r:region,
                variables: Union[str, Collection[str]],
                spin_slice: Union[int, Tuple[int, int], None] = None
                ):

        for grd in r:
            d = grd._get_daily_data(get_args(variables), spin_slice, return_time=True)

            time = [t.strftime("%Y-%m-%d") for  t in d[1]] # type: ignore
            data = d[0] # type: ignore

            new_data = {}
            for k, v in data.items(): # type: ignore
                if len(v.shape) == 1:
                    new_data[k] = np.round(v, 10)
                elif len(v.shape) == 2:
                    ny, _ = v.shape
                    _sum = np.sum(v, axis=0)
                    new_data[f"{k}_sum"] = np.round(_sum, 10)
                    for i in range(ny):
                        new_data[f"{k}_{i+1}"] = np.round(v[i,:], 10) # We assume the first axis is the time axis

            fname = f"grd_{grd.x}_{grd.y}.csv"
            df = pd.DataFrame(new_data, index=time)
            df.rename_axis('day', axis='index')
            df.to_csv(processed_data/fname, index_label='day')



def main(vrs):
    data = gridded_data.aggregate_region_data(reg, vrs, (24,25))
    # data = gridded_data.aggregate_region_data(reg, variables, spin_slice=12)
    return data



if __name__ == "__main__":

    variables_to_read = ("cue", "rnpp", "aresp", "photo", "csoil")
    data = main(variables_to_read)
    a = gridded_data.create_masked_arrays2D(data)
    table_data.make_df(reg, variables_to_read, spin_slice=(1,2))